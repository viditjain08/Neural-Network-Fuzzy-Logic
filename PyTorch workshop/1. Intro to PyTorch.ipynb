{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning with PyTorch\n",
    "\n",
    "In this notebook, you'll get introduced to [PyTorch](http://pytorch.org/), a framework for building and training neural networks. PyTorch in a lot of ways behaves like the arrays you love from Numpy. These Numpy arrays, after all, are just tensors. PyTorch takes these tensors and makes it simple to move them to GPUs for the faster processing needed when training neural networks. It also provides a module that automatically calculates gradients (for backpropagation!) and another module specifically for building neural networks. All together, PyTorch ends up being more coherent with Python and the Numpy/Scipy stack compared to TensorFlow and other frameworks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/andrej.PNG\" width=700px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras vs PyTorch\n",
    "\n",
    "* Keras is without a doubt the easier option if you want a plug & play framework: to quickly build, train, and evaluate a model, without spending much time on mathematical implementation details.\n",
    "\n",
    "* PyTorch offers a lower-level approach and more flexibility for the more mathematically-inclined users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Head to Head\n",
    "\n",
    "Consider this head-to-head comparison of how a simple convolutional network is defined in Keras and PyTorch:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(MaxPool2D())\n",
    "model.add(Conv2D(16, (3, 3), activation='relu'))\n",
    "model.add(MaxPool2D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "    \n",
    "        self.conv1 = nn.Conv2d(3, 32, 3)\n",
    "        self.conv2 = nn.Conv2d(32, 16, 3)\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 10) \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 6 * 6)\n",
    "        x = F.log_softmax(self.fc1(x), dim=-1)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "Deep Learning is based on artificial neural networks which have been around in some form since the late 1950s. The networks are built from individual parts approximating neurons, typically called units or simply \"neurons.\" Each unit has some number of weighted inputs. These weighted inputs are summed together (a linear combination) then passed through an activation function to get the unit's output.\n",
    "\n",
    "<img src=\"assets/simple_neuron.png\" width=400px>\n",
    "\n",
    "Mathematically this looks like: \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y &= f(w_1 x_1 + w_2 x_2 + b) \\\\\n",
    "y &= f\\left(\\sum_i w_i x_i +b \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "With vectors this is the dot/inner product of two vectors:\n",
    "\n",
    "$$\n",
    "h = \\begin{bmatrix}\n",
    "x_1 \\, x_2 \\cdots  x_n\n",
    "\\end{bmatrix}\n",
    "\\cdot \n",
    "\\begin{bmatrix}\n",
    "           w_1 \\\\\n",
    "           w_2 \\\\\n",
    "           \\vdots \\\\\n",
    "           w_n\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "\n",
    "It turns out neural network computations are just a bunch of linear algebra operations on *tensors*, a generalization of matrices. A vector is a 1-dimensional tensor, a matrix is a 2-dimensional tensor, an array with three indices is a 3-dimensional tensor (RGB color images for example). The fundamental data structure for neural networks are tensors and PyTorch (as well as pretty much every other deep learning framework) is built around tensors.\n",
    "\n",
    "<img src=\"assets/tensor_examples.svg\" width=600px>\n",
    "\n",
    "With the basics covered, it's time to explore how we can use PyTorch to build a simple neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    \"\"\" Sigmoid activation function \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        x: torch.Tensor\n",
    "    \"\"\"\n",
    "    return 1/(1+torch.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warm Up : NN using Numpy\n",
    "\n",
    "Before introducing PyTorch, we will first implement the network using numpy.\n",
    "\n",
    "Numpy provides an n-dimensional array object, and many functions for manipulating these arrays. Numpy is a generic framework for scientific computing; it does not know anything about computation graphs, or deep learning, or gradients. However we can easily use numpy to fit a two-layer network to random data by manually implementing the forward and backward passes through the network using numpy operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 35481441.996452734\n",
      "1 34157639.466876864\n",
      "2 37910945.62360317\n",
      "3 39115231.61626529\n",
      "4 32610395.539658725\n",
      "5 20583510.020680763\n",
      "6 10344238.966868008\n",
      "7 4795802.114984689\n",
      "8 2444995.935806998\n",
      "9 1490502.468764761\n",
      "10 1062355.8557089055\n",
      "11 831062.4484871145\n",
      "12 681147.1437995685\n",
      "13 571466.8367751485\n",
      "14 485625.82992506865\n",
      "15 416109.5707394616\n",
      "16 358757.044797295\n",
      "17 310883.7215144334\n",
      "18 270584.4294118342\n",
      "19 236442.9863176608\n",
      "20 207388.08909645298\n",
      "21 182529.47541519924\n",
      "22 161224.04289700455\n",
      "23 142824.65660724515\n",
      "24 126895.21156820342\n",
      "25 113043.73559502672\n",
      "26 100957.91428607458\n",
      "27 90382.8648505722\n",
      "28 81099.43244300855\n",
      "29 72915.2537162152\n",
      "30 65685.21717119112\n",
      "31 59286.022809866394\n",
      "32 53603.34242833019\n",
      "33 48544.7872357779\n",
      "34 44038.64164422208\n",
      "35 40009.10185146244\n",
      "36 36398.455563836666\n",
      "37 33159.19705758535\n",
      "38 30249.56788368701\n",
      "39 27630.44116403709\n",
      "40 25268.688529295272\n",
      "41 23134.861924287165\n",
      "42 21202.862216760634\n",
      "43 19452.659176338413\n",
      "44 17863.61674843206\n",
      "45 16420.406226761173\n",
      "46 15106.739800976009\n",
      "47 13910.105946701224\n",
      "48 12818.530847012145\n",
      "49 11821.978031955608\n",
      "50 10911.477082688274\n",
      "51 10078.320389530927\n",
      "52 9315.434474655322\n",
      "53 8616.338496371254\n",
      "54 7974.8001074561\n",
      "55 7385.425804671018\n",
      "56 6843.964099724767\n",
      "57 6345.582442140439\n",
      "58 5886.781775305679\n",
      "59 5464.666864913892\n",
      "60 5075.328806926738\n",
      "61 4716.0294933340865\n",
      "62 4384.407434061308\n",
      "63 4077.868335668564\n",
      "64 3794.3819952513663\n",
      "65 3532.1501422077326\n",
      "66 3289.527505393675\n",
      "67 3064.810921202099\n",
      "68 2856.6167895107665\n",
      "69 2663.451073154698\n",
      "70 2484.411693520648\n",
      "71 2318.106507689021\n",
      "72 2163.669304173255\n",
      "73 2020.2289733392886\n",
      "74 1886.9832582659994\n",
      "75 1762.994914725982\n",
      "76 1647.6682983826736\n",
      "77 1540.3574303694354\n",
      "78 1440.5188726268384\n",
      "79 1347.4233269045599\n",
      "80 1260.702024607724\n",
      "81 1179.902391122837\n",
      "82 1104.5542912046435\n",
      "83 1034.2495912783652\n",
      "84 968.6641495872267\n",
      "85 907.4825881149555\n",
      "86 850.3432561552326\n",
      "87 796.9811104242217\n",
      "88 747.1029928622729\n",
      "89 700.5038943544291\n",
      "90 656.9437958506137\n",
      "91 616.2235873315192\n",
      "92 578.1224956145156\n",
      "93 542.5097293315406\n",
      "94 509.1594667245738\n",
      "95 477.94981696090224\n",
      "96 448.7354329722591\n",
      "97 421.37935997682393\n",
      "98 395.74467888514783\n",
      "99 371.7255706467714\n",
      "100 349.24346995899424\n",
      "101 328.1648384239627\n",
      "102 308.38847459687\n",
      "103 289.8469635238944\n",
      "104 272.46131305919135\n",
      "105 256.1580664329624\n",
      "106 240.85768371464488\n",
      "107 226.50663712229345\n",
      "108 213.03293958075776\n",
      "109 200.3893146339318\n",
      "110 188.51584425667477\n",
      "111 177.36646948549992\n",
      "112 166.89396315497297\n",
      "113 157.05856830337132\n",
      "114 147.82731767398963\n",
      "115 139.1478289476554\n",
      "116 130.9892617869605\n",
      "117 123.32250934585538\n",
      "118 116.11687052613495\n",
      "119 109.34390139103745\n",
      "120 102.97565148497658\n",
      "121 96.98888726419102\n",
      "122 91.35416110706691\n",
      "123 86.05503038856853\n",
      "124 81.07325061018238\n",
      "125 76.38696550967404\n",
      "126 71.97405838454137\n",
      "127 67.82251325203289\n",
      "128 63.916253881964366\n",
      "129 60.23985667800137\n",
      "130 56.7784283830332\n",
      "131 53.51985697306011\n",
      "132 50.453818531611056\n",
      "133 47.56478589797599\n",
      "134 44.84603808961273\n",
      "135 42.28488980719961\n",
      "136 39.87225861590274\n",
      "137 37.59941774693389\n",
      "138 35.45814969974711\n",
      "139 33.44202795544098\n",
      "140 31.543173267432035\n",
      "141 29.752948304788834\n",
      "142 28.06518470316088\n",
      "143 26.4748439070146\n",
      "144 24.97624334023447\n",
      "145 23.56408027579353\n",
      "146 22.23339887425673\n",
      "147 20.97840932940888\n",
      "148 19.795984138573786\n",
      "149 18.681811535897936\n",
      "150 17.631925070029368\n",
      "151 16.64153920790267\n",
      "152 15.708140761146044\n",
      "153 14.827439840796083\n",
      "154 13.997017925876905\n",
      "155 13.213971937091774\n",
      "156 12.474828855513085\n",
      "157 11.777554769503041\n",
      "158 11.119802671595636\n",
      "159 10.499405500079565\n",
      "160 9.914130399136607\n",
      "161 9.362018335403599\n",
      "162 8.840880165616376\n",
      "163 8.348908227060154\n",
      "164 7.884604168289988\n",
      "165 7.446512393372323\n",
      "166 7.033265234368805\n",
      "167 6.642984308768911\n",
      "168 6.2746254120883735\n",
      "169 5.926895901972117\n",
      "170 5.598774084548031\n",
      "171 5.288966833776166\n",
      "172 4.996293474547685\n",
      "173 4.7200688591868944\n",
      "174 4.459197118163905\n",
      "175 4.213044486140872\n",
      "176 3.980632326091022\n",
      "177 3.7610632918181395\n",
      "178 3.55364683788259\n",
      "179 3.3578238376221288\n",
      "180 3.172917329261262\n",
      "181 2.998358422397631\n",
      "182 2.833429257180354\n",
      "183 2.6776017109100048\n",
      "184 2.5304449133175257\n",
      "185 2.3914352670946593\n",
      "186 2.2601939247953453\n",
      "187 2.1361692977546216\n",
      "188 2.018972438159146\n",
      "189 1.9082800846743726\n",
      "190 1.8036802311712767\n",
      "191 1.704926404457018\n",
      "192 1.6116193996265817\n",
      "193 1.5233962455881525\n",
      "194 1.4400448505300778\n",
      "195 1.3612931222361269\n",
      "196 1.286894126656995\n",
      "197 1.2166072446409095\n",
      "198 1.1501820665639175\n",
      "199 1.0873883635351538\n",
      "200 1.0280259896513644\n",
      "201 0.9719357510670597\n",
      "202 0.918957854709981\n",
      "203 0.8688724202708382\n",
      "204 0.8215349319732388\n",
      "205 0.7768109215268013\n",
      "206 0.7344995460064496\n",
      "207 0.6945235460963722\n",
      "208 0.6567440848588552\n",
      "209 0.6210298496312405\n",
      "210 0.5872805566340196\n",
      "211 0.5553562659127002\n",
      "212 0.525197289667283\n",
      "213 0.4966847746058978\n",
      "214 0.46971266044680915\n",
      "215 0.4442196417814487\n",
      "216 0.4201276059015794\n",
      "217 0.3973378654430507\n",
      "218 0.3758045806119442\n",
      "219 0.3554403050378053\n",
      "220 0.336175820888768\n",
      "221 0.31795844621702923\n",
      "222 0.30074241070340163\n",
      "223 0.28447370878928935\n",
      "224 0.269079302889931\n",
      "225 0.25452291587121717\n",
      "226 0.2407550550075096\n",
      "227 0.2277358657224614\n",
      "228 0.21543160031735287\n",
      "229 0.20379764087156468\n",
      "230 0.19278709434335142\n",
      "231 0.18237204505576626\n",
      "232 0.17252591696256356\n",
      "233 0.1632192102700661\n",
      "234 0.15441459735462978\n",
      "235 0.14608644088089556\n",
      "236 0.13821137046848705\n",
      "237 0.1307590766074881\n",
      "238 0.12370998652309334\n",
      "239 0.11704932116682563\n",
      "240 0.11074482319567035\n",
      "241 0.10477860085697757\n",
      "242 0.09913754234641266\n",
      "243 0.09380151004355841\n",
      "244 0.08875551482343763\n",
      "245 0.08398154861815625\n",
      "246 0.07946391642523645\n",
      "247 0.07519045655317126\n",
      "248 0.07114796360162767\n",
      "249 0.06732840425748271\n",
      "250 0.06371154495165385\n",
      "251 0.060288598334723345\n",
      "252 0.05705021384061106\n",
      "253 0.053986851356898714\n",
      "254 0.051089767294473336\n",
      "255 0.04835047742012785\n",
      "256 0.045756945936840245\n",
      "257 0.04330241430340845\n",
      "258 0.040979989379936105\n",
      "259 0.03878365995790964\n",
      "260 0.03670547372763468\n",
      "261 0.034738885988899286\n",
      "262 0.032878714124426875\n",
      "263 0.03111712828190901\n",
      "264 0.02945028914465761\n",
      "265 0.027874434303756468\n",
      "266 0.026382370028205402\n",
      "267 0.024970441168813674\n",
      "268 0.023635270800848977\n",
      "269 0.02237124490490081\n",
      "270 0.021175190739464955\n",
      "271 0.02004287129012504\n",
      "272 0.018971524641757696\n",
      "273 0.017957855327284382\n",
      "274 0.016998457082412086\n",
      "275 0.016091327792139544\n",
      "276 0.015231961921312709\n",
      "277 0.014418324340504274\n",
      "278 0.013648864745069387\n",
      "279 0.012920620362903758\n",
      "280 0.012231640759603694\n",
      "281 0.01157938243623911\n",
      "282 0.010961665307628803\n",
      "283 0.010376961050024043\n",
      "284 0.00982360538278404\n",
      "285 0.009300369973156423\n",
      "286 0.00880483119504236\n",
      "287 0.008335720693410587\n",
      "288 0.007891918687201505\n",
      "289 0.007471426589305592\n",
      "290 0.007073700769127474\n",
      "291 0.006697371365921261\n",
      "292 0.006340846961146132\n",
      "293 0.0060034602731930346\n",
      "294 0.005684131676666827\n",
      "295 0.005381870443981482\n",
      "296 0.005095746966784609\n",
      "297 0.004824850932542363\n",
      "298 0.004568360056525343\n",
      "299 0.004325618041756116\n",
      "300 0.00409592131937914\n",
      "301 0.0038783966747583543\n",
      "302 0.0036723471648766985\n",
      "303 0.0034773485822897775\n",
      "304 0.003292705215209483\n",
      "305 0.0031179868918205372\n",
      "306 0.0029526045109001054\n",
      "307 0.0027959147606366544\n",
      "308 0.0026476101571617974\n",
      "309 0.0025071509072266173\n",
      "310 0.002374237332226351\n",
      "311 0.002248416910251251\n",
      "312 0.00212922266146411\n",
      "313 0.0020164175407822687\n",
      "314 0.0019095986739878174\n",
      "315 0.0018084595079806227\n",
      "316 0.0017127079289345638\n",
      "317 0.0016219888532885912\n",
      "318 0.0015361490919037382\n",
      "319 0.0014548274467089122\n",
      "320 0.001377865679678016\n",
      "321 0.0013049873915742358\n",
      "322 0.0012359337356490042\n",
      "323 0.001170580164106331\n",
      "324 0.0011086877543168491\n",
      "325 0.0010500583265053546\n",
      "326 0.0009946082028622432\n",
      "327 0.0009420295079400271\n",
      "328 0.0008922527309511084\n",
      "329 0.0008451056953285994\n",
      "330 0.0008004823342820552\n",
      "331 0.00075823622092247\n",
      "332 0.0007182153128696997\n",
      "333 0.0006803071829583286\n",
      "334 0.000644387205294588\n",
      "335 0.000610382049209379\n",
      "336 0.0005782101805803207\n",
      "337 0.0005477060251476398\n",
      "338 0.0005188245423928369\n",
      "339 0.0004914649546064921\n",
      "340 0.0004655610866607391\n",
      "341 0.0004410419563745995\n",
      "342 0.00041780842543779186\n",
      "343 0.00039579667773538736\n",
      "344 0.0003749455355488517\n",
      "345 0.00035520128047377016\n",
      "346 0.00033650502788120355\n",
      "347 0.0003187820835199014\n",
      "348 0.0003020162238261056\n",
      "349 0.0002861297500017758\n",
      "350 0.0002710708947205153\n",
      "351 0.0002568186682389839\n",
      "352 0.00024330961536026555\n",
      "353 0.00023051586317527075\n",
      "354 0.00021840214632157137\n",
      "355 0.0002069224157914736\n",
      "356 0.00019605488030140168\n",
      "357 0.00018575162114870761\n",
      "358 0.00017599301635191255\n",
      "359 0.00016674717125805794\n",
      "360 0.00015799674301730234\n",
      "361 0.00014970706445356237\n",
      "362 0.0001418481851589701\n",
      "363 0.00013440428755878575\n",
      "364 0.00012735171334577676\n",
      "365 0.00012067054565860204\n",
      "366 0.0001143486746574523\n",
      "367 0.00010835310726659025\n",
      "368 0.000102671908494146\n",
      "369 9.72886588162504e-05\n",
      "370 9.218954722421811e-05\n",
      "371 8.736182969872583e-05\n",
      "372 8.278608543313561e-05\n",
      "373 7.845077532133302e-05\n",
      "374 7.43412084107828e-05\n",
      "375 7.044846658079212e-05\n",
      "376 6.676273234610113e-05\n",
      "377 6.32683519378253e-05\n",
      "378 5.99591567999649e-05\n",
      "379 5.682238106408207e-05\n",
      "380 5.384996753058943e-05\n",
      "381 5.103473799726561e-05\n",
      "382 4.836727287523226e-05\n",
      "383 4.584035500301654e-05\n",
      "384 4.344435201121494e-05\n",
      "385 4.1174517121596936e-05\n",
      "386 3.902428589302971e-05\n",
      "387 3.69852257317202e-05\n",
      "388 3.505390165481559e-05\n",
      "389 3.322387774237942e-05\n",
      "390 3.1489959083994084e-05\n",
      "391 2.9846954511295394e-05\n",
      "392 2.8288918264159336e-05\n",
      "393 2.6813078025816513e-05\n",
      "394 2.541469414402674e-05\n",
      "395 2.4090317287222613e-05\n",
      "396 2.283432975407382e-05\n",
      "397 2.1643760571919517e-05\n",
      "398 2.0516032805303604e-05\n",
      "399 1.944667026403419e-05\n",
      "400 1.8434037694175916e-05\n",
      "401 1.7474436311648844e-05\n",
      "402 1.6564209680922855e-05\n",
      "403 1.5701761559997425e-05\n",
      "404 1.488435089464348e-05\n",
      "405 1.4109961631102403e-05\n",
      "406 1.337582063607962e-05\n",
      "407 1.268013587969935e-05\n",
      "408 1.2020527081258234e-05\n",
      "409 1.1395345680316842e-05\n",
      "410 1.080311927754895e-05\n",
      "411 1.0241482772318645e-05\n",
      "412 9.709247126693342e-06\n",
      "413 9.204943297251932e-06\n",
      "414 8.726795164557531e-06\n",
      "415 8.273500398961695e-06\n",
      "416 7.843996773514364e-06\n",
      "417 7.43664096712723e-06\n",
      "418 7.050588865650006e-06\n",
      "419 6.684801296822494e-06\n",
      "420 6.337967515157362e-06\n",
      "421 6.009146414973862e-06\n",
      "422 5.69745576051507e-06\n",
      "423 5.402158966616453e-06\n",
      "424 5.122177256102699e-06\n",
      "425 4.856670569060663e-06\n",
      "426 4.604928368084178e-06\n",
      "427 4.366348273610987e-06\n",
      "428 4.140180913425868e-06\n",
      "429 3.9258998351783745e-06\n",
      "430 3.722556272254077e-06\n",
      "431 3.5298564147473104e-06\n",
      "432 3.347103066917098e-06\n",
      "433 3.173875186125415e-06\n",
      "434 3.009772428104082e-06\n",
      "435 2.8540592586224046e-06\n",
      "436 2.70644126214348e-06\n",
      "437 2.5664598825756972e-06\n",
      "438 2.4338373478498484e-06\n",
      "439 2.308061878376568e-06\n",
      "440 2.188817522250372e-06\n",
      "441 2.0757207912264644e-06\n",
      "442 1.968515176910718e-06\n",
      "443 1.866899796240124e-06\n",
      "444 1.7705165475059656e-06\n",
      "445 1.67910122607315e-06\n",
      "446 1.592463995888048e-06\n",
      "447 1.5102895656478505e-06\n",
      "448 1.4323771977451732e-06\n",
      "449 1.358510138450461e-06\n",
      "450 1.2884354819586982e-06\n",
      "451 1.2220132757831305e-06\n",
      "452 1.1590261257719499e-06\n",
      "453 1.099312088985227e-06\n",
      "454 1.042677550005992e-06\n",
      "455 9.88958071423922e-07\n",
      "456 9.380298655953341e-07\n",
      "457 8.897433375802025e-07\n",
      "458 8.439350080598368e-07\n",
      "459 8.004954103601491e-07\n",
      "460 7.592952367794896e-07\n",
      "461 7.202241498363742e-07\n",
      "462 6.832045374374398e-07\n",
      "463 6.480672281337187e-07\n",
      "464 6.147445774749484e-07\n",
      "465 5.831425967181359e-07\n",
      "466 5.531670347124402e-07\n",
      "467 5.247570247253829e-07\n",
      "468 4.977970917496028e-07\n",
      "469 4.722337515306244e-07\n",
      "470 4.4798002753160403e-07\n",
      "471 4.2498349798387834e-07\n",
      "472 4.0317642732241324e-07\n",
      "473 3.824850617702623e-07\n",
      "474 3.6285688909116817e-07\n",
      "475 3.4423810646006603e-07\n",
      "476 3.265882277031107e-07\n",
      "477 3.098452566114379e-07\n",
      "478 2.9395916830271006e-07\n",
      "479 2.788894874978324e-07\n",
      "480 2.6460078551833325e-07\n",
      "481 2.51045735045341e-07\n",
      "482 2.381916338393111e-07\n",
      "483 2.259969906048635e-07\n",
      "484 2.1442481216096844e-07\n",
      "485 2.0345254013966963e-07\n",
      "486 1.9303816246788066e-07\n",
      "487 1.8316045684791523e-07\n",
      "488 1.737923202837485e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "489 1.6490249388101452e-07\n",
      "490 1.5647262273503108e-07\n",
      "491 1.4847144932292937e-07\n",
      "492 1.4088396310487507e-07\n",
      "493 1.336842868245563e-07\n",
      "494 1.2685653706424737e-07\n",
      "495 1.2037820700934423e-07\n",
      "496 1.142287563227398e-07\n",
      "497 1.0839595547667614e-07\n",
      "498 1.0286296825782525e-07\n",
      "499 9.761513780425535e-08\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500): #change as per convenience\n",
    "  # Forward pass: compute predicted y\n",
    "  h = x.dot(w1)\n",
    "  h_relu = np.maximum(h, 0)\n",
    "  y_pred = h_relu.dot(w2)\n",
    "  \n",
    "  # Compute and print loss\n",
    "  loss = np.square(y_pred - y).sum()\n",
    "  print(t, loss)\n",
    "  \n",
    "  # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "  grad_y_pred = 2.0 * (y_pred - y)\n",
    "  grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "  grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "  grad_h = grad_h_relu.copy()\n",
    "  grad_h[h < 0] = 0\n",
    "  grad_w1 = x.T.dot(grad_h)\n",
    " \n",
    "  # Update weights\n",
    "  w1 -= learning_rate * grad_w1\n",
    "  w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: NN using Tensors\n",
    "\n",
    "Numpy is a great framework, but it cannot utilize GPUs to accelerate its numerical computations. For modern deep neural networks, GPUs often provide speedups of 50x or greater, so unfortunately numpy won't be enough for modern deep learning.\n",
    "\n",
    "Here we introduce the most fundamental PyTorch concept: the Tensor. A PyTorch Tensor is conceptually identical to a numpy array: a Tensor is an n-dimensional array, and PyTorch provides many functions for operating on these Tensors. Any computation you might want to perform with numpy can also be accomplished with PyTorch Tensors; you should think of them as a generic tool for scientific computing.\n",
    "\n",
    "However unlike numpy, PyTorch Tensors can utilize GPUs to accelerate their numeric computations. To run a PyTorch Tensor on GPU, you use the device argument when constructing a Tensor to place the Tensor on a GPU.\n",
    "\n",
    "Here we use PyTorch Tensors to fit a two-layer network to random data. Like the numpy example above we manually implement the forward and backward passes through the network, using operations on PyTorch Tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 31232918.0\n",
      "1 24215340.0\n",
      "2 20549676.0\n",
      "3 17316538.0\n",
      "4 13790895.0\n",
      "5 10246608.0\n",
      "6 7174554.5\n",
      "7 4858219.0\n",
      "8 3268973.5\n",
      "9 2238554.0\n",
      "10 1583090.25\n",
      "11 1164594.75\n",
      "12 890625.25\n",
      "13 705201.4375\n",
      "14 574666.8125\n",
      "15 479003.53125\n",
      "16 406049.96875\n",
      "17 348635.9375\n",
      "18 302232.25\n",
      "19 263982.6875\n",
      "20 231951.75\n",
      "21 204819.234375\n",
      "22 181637.0625\n",
      "23 161655.984375\n",
      "24 144344.75\n",
      "25 129247.6640625\n",
      "26 116025.0625\n",
      "27 104391.8828125\n",
      "28 94119.0859375\n",
      "29 85033.28125\n",
      "30 76971.453125\n",
      "31 69792.4296875\n",
      "32 63383.53125\n",
      "33 57650.25\n",
      "34 52514.578125\n",
      "35 47900.35546875\n",
      "36 43747.26171875\n",
      "37 40003.05859375\n",
      "38 36627.4609375\n",
      "39 33575.1875\n",
      "40 30811.5234375\n",
      "41 28311.53515625\n",
      "42 26042.01953125\n",
      "43 23978.859375\n",
      "44 22099.10546875\n",
      "45 20384.466796875\n",
      "46 18818.0390625\n",
      "47 17386.59765625\n",
      "48 16075.93359375\n",
      "49 14875.306640625\n",
      "50 13774.28125\n",
      "51 12763.630859375\n",
      "52 11834.3759765625\n",
      "53 10979.779296875\n",
      "54 10193.0732421875\n",
      "55 9468.201171875\n",
      "56 8799.7119140625\n",
      "57 8182.93505859375\n",
      "58 7613.42578125\n",
      "59 7087.001953125\n",
      "60 6600.3603515625\n",
      "61 6150.08984375\n",
      "62 5733.22265625\n",
      "63 5346.8544921875\n",
      "64 4988.58642578125\n",
      "65 4656.4736328125\n",
      "66 4348.37109375\n",
      "67 4062.255859375\n",
      "68 3796.4072265625\n",
      "69 3549.373046875\n",
      "70 3319.525634765625\n",
      "71 3105.63916015625\n",
      "72 2906.5107421875\n",
      "73 2721.021728515625\n",
      "74 2548.22119140625\n",
      "75 2387.146728515625\n",
      "76 2236.905029296875\n",
      "77 2096.76318359375\n",
      "78 1966.01025390625\n",
      "79 1843.8228759765625\n",
      "80 1729.7039794921875\n",
      "81 1623.05615234375\n",
      "82 1523.3720703125\n",
      "83 1430.15869140625\n",
      "84 1342.963623046875\n",
      "85 1261.393798828125\n",
      "86 1185.048095703125\n",
      "87 1113.5771484375\n",
      "88 1046.6351318359375\n",
      "89 983.9066162109375\n",
      "90 925.1119384765625\n",
      "91 869.9981079101562\n",
      "92 818.3328247070312\n",
      "93 769.8658447265625\n",
      "94 724.4046630859375\n",
      "95 681.745849609375\n",
      "96 641.7052612304688\n",
      "97 604.1214599609375\n",
      "98 568.8275756835938\n",
      "99 535.6973266601562\n",
      "100 504.6196594238281\n",
      "101 475.41607666015625\n",
      "102 447.9591064453125\n",
      "103 422.149169921875\n",
      "104 397.87823486328125\n",
      "105 375.0561828613281\n",
      "106 353.605712890625\n",
      "107 333.4228820800781\n",
      "108 314.42608642578125\n",
      "109 296.5533447265625\n",
      "110 279.7319641113281\n",
      "111 263.905517578125\n",
      "112 248.99276733398438\n",
      "113 234.95565795898438\n",
      "114 221.73391723632812\n",
      "115 209.2841796875\n",
      "116 197.55319213867188\n",
      "117 186.5006103515625\n",
      "118 176.08316040039062\n",
      "119 166.266357421875\n",
      "120 157.01492309570312\n",
      "121 148.29605102539062\n",
      "122 140.06884765625\n",
      "123 132.31396484375\n",
      "124 125.0008316040039\n",
      "125 118.10592651367188\n",
      "126 111.6012191772461\n",
      "127 105.46515655517578\n",
      "128 99.6733627319336\n",
      "129 94.21022033691406\n",
      "130 89.0525131225586\n",
      "131 84.185546875\n",
      "132 79.5904541015625\n",
      "133 75.25382995605469\n",
      "134 71.1591796875\n",
      "135 67.29377746582031\n",
      "136 63.64209747314453\n",
      "137 60.19292449951172\n",
      "138 56.93608093261719\n",
      "139 53.85887145996094\n",
      "140 50.95272445678711\n",
      "141 48.20659637451172\n",
      "142 45.61164855957031\n",
      "143 43.1598014831543\n",
      "144 40.842933654785156\n",
      "145 38.65240478515625\n",
      "146 36.58191680908203\n",
      "147 34.62580871582031\n",
      "148 32.77606964111328\n",
      "149 31.027040481567383\n",
      "150 29.372825622558594\n",
      "151 27.80875587463379\n",
      "152 26.330482482910156\n",
      "153 24.932334899902344\n",
      "154 23.60904312133789\n",
      "155 22.35839080810547\n",
      "156 21.1743221282959\n",
      "157 20.055057525634766\n",
      "158 18.995492935180664\n",
      "159 17.99296760559082\n",
      "160 17.044309616088867\n",
      "161 16.146926879882812\n",
      "162 15.297785758972168\n",
      "163 14.493956565856934\n",
      "164 13.733327865600586\n",
      "165 13.013254165649414\n",
      "166 12.331525802612305\n",
      "167 11.686238288879395\n",
      "168 11.075511932373047\n",
      "169 10.497209548950195\n",
      "170 9.949382781982422\n",
      "171 9.430695533752441\n",
      "172 8.939229011535645\n",
      "173 8.474115371704102\n",
      "174 8.033706665039062\n",
      "175 7.616830825805664\n",
      "176 7.221283912658691\n",
      "177 6.846987247467041\n",
      "178 6.492719650268555\n",
      "179 6.156379699707031\n",
      "180 5.838132858276367\n",
      "181 5.536818504333496\n",
      "182 5.2509355545043945\n",
      "183 4.980259895324707\n",
      "184 4.723490238189697\n",
      "185 4.480524063110352\n",
      "186 4.249973297119141\n",
      "187 4.031754016876221\n",
      "188 3.8247711658477783\n",
      "189 3.62855863571167\n",
      "190 3.442653179168701\n",
      "191 3.266329765319824\n",
      "192 3.0989980697631836\n",
      "193 2.9405651092529297\n",
      "194 2.790402889251709\n",
      "195 2.6478476524353027\n",
      "196 2.512789726257324\n",
      "197 2.3846893310546875\n",
      "198 2.2632601261138916\n",
      "199 2.1480541229248047\n",
      "200 2.038815975189209\n",
      "201 1.9352318048477173\n",
      "202 1.8370184898376465\n",
      "203 1.7437257766723633\n",
      "204 1.6553212404251099\n",
      "205 1.5713858604431152\n",
      "206 1.4919228553771973\n",
      "207 1.4165347814559937\n",
      "208 1.3449170589447021\n",
      "209 1.2770411968231201\n",
      "210 1.2124137878417969\n",
      "211 1.1512706279754639\n",
      "212 1.0933138132095337\n",
      "213 1.038228988647461\n",
      "214 0.9859316945075989\n",
      "215 0.9363549947738647\n",
      "216 0.889230489730835\n",
      "217 0.8445957899093628\n",
      "218 0.8021365404129028\n",
      "219 0.7619078159332275\n",
      "220 0.7238140106201172\n",
      "221 0.6875712871551514\n",
      "222 0.6530765295028687\n",
      "223 0.6203912496566772\n",
      "224 0.5893345475196838\n",
      "225 0.5599297285079956\n",
      "226 0.5320125222206116\n",
      "227 0.5054076910018921\n",
      "228 0.4801916182041168\n",
      "229 0.45632365345954895\n",
      "230 0.4334803819656372\n",
      "231 0.4119057059288025\n",
      "232 0.391449898481369\n",
      "233 0.37196630239486694\n",
      "234 0.3534948229789734\n",
      "235 0.33594006299972534\n",
      "236 0.3192487955093384\n",
      "237 0.30342400074005127\n",
      "238 0.28842058777809143\n",
      "239 0.27413469552993774\n",
      "240 0.26052191853523254\n",
      "241 0.247628316283226\n",
      "242 0.2354026436805725\n",
      "243 0.22374486923217773\n",
      "244 0.2126820832490921\n",
      "245 0.20220693945884705\n",
      "246 0.19222521781921387\n",
      "247 0.182785302400589\n",
      "248 0.1737549901008606\n",
      "249 0.1652033030986786\n",
      "250 0.1570739448070526\n",
      "251 0.14932209253311157\n",
      "252 0.1420041173696518\n",
      "253 0.13504035770893097\n",
      "254 0.12839847803115845\n",
      "255 0.1221046969294548\n",
      "256 0.11610869318246841\n",
      "257 0.11040250957012177\n",
      "258 0.1049891784787178\n",
      "259 0.09985066950321198\n",
      "260 0.09499294310808182\n",
      "261 0.09031151980161667\n",
      "262 0.08591081202030182\n",
      "263 0.08171955496072769\n",
      "264 0.07773377746343613\n",
      "265 0.07391786575317383\n",
      "266 0.07032418251037598\n",
      "267 0.0668838620185852\n",
      "268 0.06363506615161896\n",
      "269 0.060543131083250046\n",
      "270 0.05759637802839279\n",
      "271 0.05480426549911499\n",
      "272 0.05215435102581978\n",
      "273 0.04960966110229492\n",
      "274 0.04721388965845108\n",
      "275 0.04491259902715683\n",
      "276 0.04275723546743393\n",
      "277 0.040682993829250336\n",
      "278 0.038714103400707245\n",
      "279 0.03684243932366371\n",
      "280 0.035051379352808\n",
      "281 0.03336883336305618\n",
      "282 0.031772129237651825\n",
      "283 0.030243810266256332\n",
      "284 0.028785159811377525\n",
      "285 0.027398686856031418\n",
      "286 0.026080090552568436\n",
      "287 0.02483556978404522\n",
      "288 0.02363509312272072\n",
      "289 0.02250354178249836\n",
      "290 0.021414965391159058\n",
      "291 0.020389273762702942\n",
      "292 0.019418083131313324\n",
      "293 0.01849653385579586\n",
      "294 0.017615661025047302\n",
      "295 0.0167714674025774\n",
      "296 0.015964718535542488\n",
      "297 0.015209288336336613\n",
      "298 0.014492504298686981\n",
      "299 0.013807370327413082\n",
      "300 0.013157851062715054\n",
      "301 0.012531168758869171\n",
      "302 0.011941362172365189\n",
      "303 0.011378969997167587\n",
      "304 0.010849634185433388\n",
      "305 0.010343187488615513\n",
      "306 0.009860312566161156\n",
      "307 0.009397519752383232\n",
      "308 0.008961194194853306\n",
      "309 0.00853758305311203\n",
      "310 0.008148737251758575\n",
      "311 0.007773439399898052\n",
      "312 0.007417322136461735\n",
      "313 0.007072834298014641\n",
      "314 0.0067509859800338745\n",
      "315 0.006441759876906872\n",
      "316 0.006149531342089176\n",
      "317 0.005868199747055769\n",
      "318 0.005605701357126236\n",
      "319 0.005350207444280386\n",
      "320 0.005110793747007847\n",
      "321 0.004885170608758926\n",
      "322 0.004665040411055088\n",
      "323 0.004456548951566219\n",
      "324 0.004259031265974045\n",
      "325 0.0040739355608820915\n",
      "326 0.0038942755199968815\n",
      "327 0.00372200645506382\n",
      "328 0.0035602725110948086\n",
      "329 0.0034064229112118483\n",
      "330 0.0032599514815956354\n",
      "331 0.0031218035146594048\n",
      "332 0.0029842224903404713\n",
      "333 0.0028563030064105988\n",
      "334 0.002734920009970665\n",
      "335 0.002620114479213953\n",
      "336 0.002511634025722742\n",
      "337 0.0024040057323873043\n",
      "338 0.0023046222049742937\n",
      "339 0.0022107032127678394\n",
      "340 0.0021199574694037437\n",
      "341 0.0020324941724538803\n",
      "342 0.0019482331117615104\n",
      "343 0.0018693916499614716\n",
      "344 0.001793918083421886\n",
      "345 0.001723283901810646\n",
      "346 0.0016541536897420883\n",
      "347 0.0015900826547294855\n",
      "348 0.0015262472443282604\n",
      "349 0.001467601046897471\n",
      "350 0.001411843579262495\n",
      "351 0.0013572354800999165\n",
      "352 0.0013062121579423547\n",
      "353 0.0012586894445121288\n",
      "354 0.001209377427585423\n",
      "355 0.001164578483439982\n",
      "356 0.0011230290401726961\n",
      "357 0.0010810759849846363\n",
      "358 0.0010414592688903213\n",
      "359 0.001004922902211547\n",
      "360 0.0009692022576928139\n",
      "361 0.0009340782416984439\n",
      "362 0.0009020438301376998\n",
      "363 0.0008691652910783887\n",
      "364 0.0008389954455196857\n",
      "365 0.0008104701992124319\n",
      "366 0.0007825811626389623\n",
      "367 0.0007572524482384324\n",
      "368 0.0007303694146685302\n",
      "369 0.000705694139469415\n",
      "370 0.0006829339545220137\n",
      "371 0.0006598422769457102\n",
      "372 0.0006391947972588241\n",
      "373 0.0006183000514283776\n",
      "374 0.0005978926783427596\n",
      "375 0.0005789811257272959\n",
      "376 0.0005610209191218019\n",
      "377 0.000543651229236275\n",
      "378 0.0005267877131700516\n",
      "379 0.0005096529494039714\n",
      "380 0.0004932688898406923\n",
      "381 0.00047923001693561673\n",
      "382 0.0004648618632927537\n",
      "383 0.00044955610064789653\n",
      "384 0.0004373954434413463\n",
      "385 0.0004244489246048033\n",
      "386 0.000411758606787771\n",
      "387 0.00039941532304510474\n",
      "388 0.00038848890108056366\n",
      "389 0.0003766301670111716\n",
      "390 0.000366284599294886\n",
      "391 0.00035576685331761837\n",
      "392 0.00034605988184921443\n",
      "393 0.0003358862013556063\n",
      "394 0.00032699358416721225\n",
      "395 0.00031820888398215175\n",
      "396 0.0003097320441156626\n",
      "397 0.00030148919904604554\n",
      "398 0.00029342109337449074\n",
      "399 0.0002863849513232708\n",
      "400 0.0002776444598566741\n",
      "401 0.00027118969592265785\n",
      "402 0.0002637938887346536\n",
      "403 0.000257240841165185\n",
      "404 0.00025086855748668313\n",
      "405 0.0002447505248710513\n",
      "406 0.00023867397976573557\n",
      "407 0.00023259702720679343\n",
      "408 0.00022723297297488898\n",
      "409 0.00022170686861500144\n",
      "410 0.0002170874795410782\n",
      "411 0.00021122698672115803\n",
      "412 0.00020586137543432415\n",
      "413 0.00020107637101318687\n",
      "414 0.00019671261543408036\n",
      "415 0.0001922386873047799\n",
      "416 0.000188185615115799\n",
      "417 0.00018399686086922884\n",
      "418 0.00017986354941967875\n",
      "419 0.00017542655405122787\n",
      "420 0.0001712060475256294\n",
      "421 0.00016788605717010796\n",
      "422 0.00016441830666735768\n",
      "423 0.0001607660815352574\n",
      "424 0.00015752128092572093\n",
      "425 0.0001544751285109669\n",
      "426 0.00015147589147090912\n",
      "427 0.00014809318236075342\n",
      "428 0.000145381098263897\n",
      "429 0.0001425504742655903\n",
      "430 0.00013941444922238588\n",
      "431 0.00013608906010631472\n",
      "432 0.00013374756963457912\n",
      "433 0.00013115169713273644\n",
      "434 0.00012862241419497877\n",
      "435 0.00012580858310684562\n",
      "436 0.0001235906092915684\n",
      "437 0.00012086147035006434\n",
      "438 0.00011864702537423\n",
      "439 0.00011669642117340118\n",
      "440 0.00011448086297605187\n",
      "441 0.00011198453285032883\n",
      "442 0.00011021492537111044\n",
      "443 0.00010781368473544717\n",
      "444 0.00010588559962343425\n",
      "445 0.00010365608613938093\n",
      "446 0.00010178890079259872\n",
      "447 9.997812594519928e-05\n",
      "448 9.802689601201564e-05\n",
      "449 9.647611295804381e-05\n",
      "450 9.480513108428568e-05\n",
      "451 9.343466081190854e-05\n",
      "452 9.189874981530011e-05\n",
      "453 8.990656351670623e-05\n",
      "454 8.840541704557836e-05\n",
      "455 8.694357529748231e-05\n",
      "456 8.579623681725934e-05\n",
      "457 8.405737753491849e-05\n",
      "458 8.288529352284968e-05\n",
      "459 8.17293839645572e-05\n",
      "460 8.022644033189863e-05\n",
      "461 7.896480383351445e-05\n",
      "462 7.759734580758959e-05\n",
      "463 7.64061333029531e-05\n",
      "464 7.502319931518286e-05\n",
      "465 7.358168659266084e-05\n",
      "466 7.267165347002447e-05\n",
      "467 7.153225305955857e-05\n",
      "468 7.05219863448292e-05\n",
      "469 6.949988892301917e-05\n",
      "470 6.833348015788943e-05\n",
      "471 6.73190806992352e-05\n",
      "472 6.644879613304511e-05\n",
      "473 6.545245560118929e-05\n",
      "474 6.44427927909419e-05\n",
      "475 6.364072032738477e-05\n",
      "476 6.2492661527358e-05\n",
      "477 6.146614032331854e-05\n",
      "478 6.082794425310567e-05\n",
      "479 5.9858375607291237e-05\n",
      "480 5.892835179110989e-05\n",
      "481 5.8084824559045956e-05\n",
      "482 5.710084951715544e-05\n",
      "483 5.6453987781424075e-05\n",
      "484 5.570593930315226e-05\n",
      "485 5.5024134780978784e-05\n",
      "486 5.414316547103226e-05\n",
      "487 5.334079105523415e-05\n",
      "488 5.2816456445725635e-05\n",
      "489 5.195408812141977e-05\n",
      "490 5.115907697472721e-05\n",
      "491 5.049764149589464e-05\n",
      "492 4.967791028320789e-05\n",
      "493 4.932655065204017e-05\n",
      "494 4.8499758122488856e-05\n",
      "495 4.78018591820728e-05\n",
      "496 4.73078980576247e-05\n",
      "497 4.647836976801045e-05\n",
      "498 4.592563345795497e-05\n",
      "499 4.538234497886151e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cpu')\n",
    "device = torch.device('cuda') # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H, device=device)\n",
    "w2 = torch.randn(H, D_out, device=device)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "  # Forward pass: compute predicted y\n",
    "  h = x.mm(w1)\n",
    "  h_relu = h.clamp(min=0)\n",
    "  y_pred = h_relu.mm(w2)\n",
    "\n",
    "  # Compute and print loss; loss is a scalar, and is stored in a PyTorch Tensor\n",
    "  # of shape (); we can get its value as a Python number with loss.item().\n",
    "  loss = (y_pred - y).pow(2).sum()\n",
    "  print(t, loss.item())\n",
    "\n",
    "  # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "  grad_y_pred = 2.0 * (y_pred - y)\n",
    "  grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "  grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "  grad_h = grad_h_relu.clone()\n",
    "  grad_h[h < 0] = 0\n",
    "  grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "  # Update weights using gradient descent\n",
    "  w1 -= learning_rate * grad_w1\n",
    "  w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Graph and Autograd\n",
    "\n",
    "In the above examples, we had to manually implement both the forward and backward passes of our neural network. Manually implementing the backward pass is not a big deal for a small two-layer network, but can quickly get very hairy for large complex networks.\n",
    "\n",
    "Thankfully, we can use automatic differentiation to automate the computation of backward passes in neural networks. The autograd package in PyTorch provides exactly this functionality. When using autograd, the forward pass of your network will define a computational graph; nodes in the graph will be Tensors, and edges will be functions that produce output Tensors from input Tensors. Backpropagating through this graph then allows you to easily compute gradients.\n",
    "\n",
    "This sounds complicated, it's pretty simple to use in practice. If we want to compute gradients with respect to some Tensor, then we set requires_grad=True when constructing that Tensor. Any PyTorch operations on that Tensor will cause a computational graph to be constructed, allowing us to later perform backpropagation through the graph. If x is a Tensor with requires_grad=True, then after backpropagation x.grad will be another Tensor holding the gradient of x with respect to some scalar value.\n",
    "\n",
    "Sometimes you may wish to prevent PyTorch from building computational graphs when performing certain operations on Tensors with requires_grad=True; for example we usually don't want to backpropagate through the weight update steps when training a neural network. In such scenarios we can use the torch.no_grad() context manager to prevent the construction of a computational graph.\n",
    "\n",
    "Here we use PyTorch Tensors and autograd to implement our two-layer network; now we no longer need to manually implement the backward pass through the network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/autograd1.PNG\" width=700px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/variable1.PNG\" width=700px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of w1 w.r.t to L: -36.0\n",
      "Gradient of w2 w.r.t to L: -28.0\n",
      "Gradient of w3 w.r.t to L: -8.0\n",
      "Gradient of w4 w.r.t to L: -20.0\n"
     ]
    }
   ],
   "source": [
    "from torch import FloatTensor\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "# Define the leaf nodes\n",
    "a = Variable(FloatTensor([4]))\n",
    "\n",
    "weights = [Variable(FloatTensor([i]), requires_grad=True) for i in (2, 5, 9, 7)]\n",
    "\n",
    "# unpack the weights for nicer assignment\n",
    "w1, w2, w3, w4 = weights\n",
    "\n",
    "b = w1 * a\n",
    "c = w2 * a\n",
    "d = w3 * b + w4 * c\n",
    "L = (10 - d)\n",
    "\n",
    "L.backward()\n",
    "\n",
    "for index, weight in enumerate(weights, start=1):\n",
    "    gradient, *_ = weight.grad.data\n",
    "    print(f\"Gradient of w{index} w.r.t to L: {gradient}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
